{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad837ca6-1364-451b-87c5-53f79def4484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de15d126-a51e-4783-9ede-bbd569c72eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67aafe9b-beaf-4360-b340-1341dee88217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Advanced Performance Optimization for Professional Chatbot\n",
    "\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class PerformanceOptimizedRAG:\n",
    "    \"\"\"Enhanced RAG with caching and performance optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = get_databricks_client()\n",
    "        self.embedding_model = load_embedding_model()\n",
    "        self.professional_data = load_professional_data()\n",
    "        \n",
    "        # Performance optimization components\n",
    "        self.response_cache = {}\n",
    "        self.embedding_cache = {}\n",
    "        self.context_cache = {}\n",
    "        self.query_analytics = []\n",
    "        \n",
    "        # Pre-compute optimized document chunks\n",
    "        self.document_chunks = self._create_optimized_chunks()\n",
    "        self.embeddings = self._create_cached_embeddings()\n",
    "        \n",
    "        # Initialize query classifier\n",
    "        self.query_classifier = self._initialize_query_classifier()\n",
    "    \n",
    "    def _create_optimized_chunks(self) -> List[Dict]:\n",
    "        \"\"\"Create smaller, focused document chunks for better retrieval\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Split experience into granular chunks\n",
    "        for exp in self.professional_data[\"experience\"]:\n",
    "            # Role overview chunk\n",
    "            chunks.append({\n",
    "                \"content\": f\"Role: {exp['role']} at {exp['company']} ({exp['duration']})\",\n",
    "                \"type\": \"role\",\n",
    "                \"importance\": 0.9,\n",
    "                \"tokens\": len(f\"{exp['role']} {exp['company']}\") // 4\n",
    "            })\n",
    "            \n",
    "            # Responsibilities chunk\n",
    "            chunks.append({\n",
    "                \"content\": f\"Responsibilities: {exp['description']}\",\n",
    "                \"type\": \"responsibilities\", \n",
    "                \"importance\": 0.8,\n",
    "                \"tokens\": len(exp['description']) // 4\n",
    "            })\n",
    "            \n",
    "            # Skills chunk\n",
    "            chunks.append({\n",
    "                \"content\": f\"Skills used: {', '.join(exp['skills'])}\",\n",
    "                \"type\": \"skills\",\n",
    "                \"importance\": 0.7,\n",
    "                \"tokens\": len(', '.join(exp['skills'])) // 4\n",
    "            })\n",
    "        \n",
    "        # Project chunks\n",
    "        for proj in self.professional_data[\"projects\"]:\n",
    "            chunks.append({\n",
    "                \"content\": f\"Project: {proj['name']} - {proj['description']}\",\n",
    "                \"type\": \"project\",\n",
    "                \"importance\": 0.9,\n",
    "                \"tokens\": len(f\"{proj['name']} {proj['description']}\") // 4\n",
    "            })\n",
    "            \n",
    "            chunks.append({\n",
    "                \"content\": f\"Tech stack: {', '.join(proj['tech_stack'])}. Impact: {proj['impact']}\",\n",
    "                \"type\": \"project_tech\",\n",
    "                \"importance\": 0.8,\n",
    "                \"tokens\": len(f\"{proj['tech_stack']} {proj['impact']}\") // 4\n",
    "            })\n",
    "        \n",
    "        # Skill category chunks\n",
    "        for category, skills in self.professional_data[\"skills\"].items():\n",
    "            chunks.append({\n",
    "                \"content\": f\"{category.replace('_', ' ').title()}: {', '.join(skills)}\",\n",
    "                \"type\": \"skills_category\",\n",
    "                \"importance\": 0.6,\n",
    "                \"tokens\": len(', '.join(skills)) // 4\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_cached_embeddings(self) -> List[np.ndarray]:\n",
    "        \"\"\"Create embeddings with caching for performance\"\"\"\n",
    "        cache_key = \"document_embeddings\"\n",
    "        \n",
    "        if cache_key in self.embedding_cache:\n",
    "            return self.embedding_cache[cache_key]\n",
    "        \n",
    "        documents = [chunk[\"content\"] for chunk in self.document_chunks]\n",
    "        \n",
    "        if self.embedding_model == \"databricks\" and self.client:\n",
    "            try:\n",
    "                response = self.client.predict(\n",
    "                    endpoint=Config.EMBEDDING_MODEL,\n",
    "                    inputs={\"input\": documents}\n",
    "                )\n",
    "                embeddings = [np.array(emb) for emb in response[\"data\"]]\n",
    "            except:\n",
    "                model = SentenceTransformer(Config.FALLBACK_EMBEDDING_MODEL)\n",
    "                embeddings = model.encode(documents)\n",
    "        else:\n",
    "            embeddings = self.embedding_model.encode(documents)\n",
    "        \n",
    "        self.embedding_cache[cache_key] = embeddings\n",
    "        return embeddings\n",
    "    \n",
    "    def _initialize_query_classifier(self) -> Dict:\n",
    "        \"\"\"Initialize query classifier for smart context selection\"\"\"\n",
    "        return {\n",
    "            \"project_keywords\": [\"project\", \"built\", \"developed\", \"created\", \"implemented\", \"designed\"],\n",
    "            \"skill_keywords\": [\"skill\", \"technology\", \"tool\", \"programming\", \"language\", \"framework\"],\n",
    "            \"experience_keywords\": [\"experience\", \"role\", \"job\", \"work\", \"position\", \"career\"],\n",
    "            \"databricks_keywords\": [\"databricks\", \"delta\", \"mlflow\", \"spark\", \"lakehouse\", \"unity\"]\n",
    "        }\n",
    "    \n",
    "    def classify_query(self, query: str) -> str:\n",
    "        \"\"\"Classify query type for optimized context retrieval\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Priority classification\n",
    "        if any(word in query_lower for word in self.query_classifier[\"databricks_keywords\"]):\n",
    "            return \"databricks\"\n",
    "        elif any(word in query_lower for word in self.query_classifier[\"project_keywords\"]):\n",
    "            return \"project\"\n",
    "        elif any(word in query_lower for word in self.query_classifier[\"skill_keywords\"]):\n",
    "            return \"skill\"\n",
    "        elif any(word in query_lower for word in self.query_classifier[\"experience_keywords\"]):\n",
    "            return \"experience\"\n",
    "        else:\n",
    "            return \"general\"\n",
    "    \n",
    "    def get_smart_context(self, query: str, max_tokens: int = 1000) -> str:\n",
    "        \"\"\"Get optimized context based on query classification\"\"\"\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = hashlib.md5(f\"{query}_{max_tokens}\".encode()).hexdigest()\n",
    "        if cache_key in self.context_cache:\n",
    "            return self.context_cache[cache_key]\n",
    "        \n",
    "        query_type = self.classify_query(query)\n",
    "        \n",
    "        # Get query embedding\n",
    "        if self.embedding_model == \"databricks\" and self.client:\n",
    "            try:\n",
    "                query_response = self.client.predict(\n",
    "                    endpoint=Config.EMBEDDING_MODEL,\n",
    "                    inputs={\"input\": [query]}\n",
    "                )\n",
    "                query_embedding = np.array(query_response[\"data\"][0])\n",
    "            except:\n",
    "                model = SentenceTransformer(Config.FALLBACK_EMBEDDING_MODEL)\n",
    "                query_embedding = model.encode([query])[0]\n",
    "        else:\n",
    "            query_embedding = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        # Calculate similarities with importance weighting\n",
    "        scored_chunks = []\n",
    "        for i, chunk in enumerate(self.document_chunks):\n",
    "            embedding = self.embeddings[i]\n",
    "            similarity = np.dot(query_embedding, embedding) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(embedding)\n",
    "            )\n",
    "            \n",
    "            # Apply importance and type weighting\n",
    "            importance_weight = chunk[\"importance\"]\n",
    "            type_weight = self._get_type_weight(chunk[\"type\"], query_type)\n",
    "            \n",
    "            final_score = similarity * importance_weight * type_weight\n",
    "            scored_chunks.append((final_score, chunk))\n",
    "        \n",
    "        # Sort by score and select top chunks within token limit\n",
    "        scored_chunks.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        selected_chunks = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for score, chunk in scored_chunks:\n",
    "            if total_tokens + chunk[\"tokens\"] <= max_tokens:\n",
    "                selected_chunks.append(chunk[\"content\"])\n",
    "                total_tokens += chunk[\"tokens\"]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        context = \"\\n\\n\".join(selected_chunks)\n",
    "        \n",
    "        # Cache the result\n",
    "        self.context_cache[cache_key] = context\n",
    "        return context\n",
    "    \n",
    "    def _get_type_weight(self, chunk_type: str, query_type: str) -> float:\n",
    "        \"\"\"Get type-specific weighting for better relevance\"\"\"\n",
    "        weights = {\n",
    "            \"databricks\": {\n",
    "                \"project\": 1.2,\n",
    "                \"project_tech\": 1.3,\n",
    "                \"skills_category\": 1.1,\n",
    "                \"role\": 1.0,\n",
    "                \"responsibilities\": 1.1\n",
    "            },\n",
    "            \"project\": {\n",
    "                \"project\": 1.3,\n",
    "                \"project_tech\": 1.2,\n",
    "                \"responsibilities\": 1.0,\n",
    "                \"role\": 0.8,\n",
    "                \"skills_category\": 0.9\n",
    "            },\n",
    "            \"skill\": {\n",
    "                \"skills_category\": 1.3,\n",
    "                \"project_tech\": 1.1,\n",
    "                \"project\": 1.0,\n",
    "                \"responsibilities\": 0.9,\n",
    "                \"role\": 0.7\n",
    "            },\n",
    "            \"experience\": {\n",
    "                \"role\": 1.3,\n",
    "                \"responsibilities\": 1.2,\n",
    "                \"project\": 1.0,\n",
    "                \"skills_category\": 0.8,\n",
    "                \"project_tech\": 0.9\n",
    "            },\n",
    "            \"general\": {\n",
    "                \"role\": 1.0,\n",
    "                \"responsibilities\": 1.0,\n",
    "                \"project\": 1.0,\n",
    "                \"skills_category\": 1.0,\n",
    "                \"project_tech\": 1.0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return weights.get(query_type, {}).get(chunk_type, 1.0)\n",
    "    \n",
    "    def get_cached_response(self, query: str, chat_history: List[Dict]) -> str:\n",
    "        \"\"\"Check for cached responses to similar queries\"\"\"\n",
    "        \n",
    "        # Create cache key from query and recent history\n",
    "        recent_history = chat_history[-2:] if len(chat_history) > 2 else chat_history\n",
    "        cache_key = hashlib.md5(f\"{query}_{str(recent_history)}\".encode()).hexdigest()\n",
    "        \n",
    "        if cache_key in self.response_cache:\n",
    "            cached_response, timestamp = self.response_cache[cache_key]\n",
    "            \n",
    "            # Check if cache is still valid (30 minutes)\n",
    "            if datetime.now() - timestamp < timedelta(minutes=30):\n",
    "                return cached_response\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def cache_response(self, query: str, chat_history: List[Dict], response: str):\n",
    "        \"\"\"Cache response for future use\"\"\"\n",
    "        recent_history = chat_history[-2:] if len(chat_history) > 2 else chat_history\n",
    "        cache_key = hashlib.md5(f\"{query}_{str(recent_history)}\".encode()).hexdigest()\n",
    "        \n",
    "        self.response_cache[cache_key] = (response, datetime.now())\n",
    "        \n",
    "        # Limit cache size\n",
    "        if len(self.response_cache) > 100:\n",
    "            # Remove oldest entries\n",
    "            oldest_key = min(self.response_cache.keys(), \n",
    "                           key=lambda k: self.response_cache[k][1])\n",
    "            del self.response_cache[oldest_key]\n",
    "    \n",
    "    def track_query_analytics(self, query: str, response_time: float, context_tokens: int):\n",
    "        \"\"\"Track performance analytics\"\"\"\n",
    "        self.query_analytics.append({\n",
    "            \"query\": query,\n",
    "            \"response_time\": response_time,\n",
    "            \"context_tokens\": context_tokens,\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        # Keep only last 100 queries\n",
    "        if len(self.query_analytics) > 100:\n",
    "            self.query_analytics = self.query_analytics[-100:]\n",
    "\n",
    "class OptimizedChatBot:\n",
    "    \"\"\"Performance-optimized chatbot with caching and smart context management\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = get_databricks_client()\n",
    "        self.rag = PerformanceOptimizedRAG()\n",
    "        \n",
    "    def generate_response(self, user_query: str, chat_history: List[Dict]) -> str:\n",
    "        \"\"\"Generate optimized response with caching and performance tracking\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check cache first\n",
    "        cached_response = self.rag.get_cached_response(user_query, chat_history)\n",
    "        if cached_response:\n",
    "            return cached_response\n",
    "        \n",
    "        if not self.client:\n",
    "            return \"I'm sorry, but I'm having trouble connecting to Databricks services right now.\"\n",
    "        \n",
    "        # Get smart context\n",
    "        context = self.rag.get_smart_context(user_query, max_tokens=1000)\n",
    "        context_tokens = len(context) // 4\n",
    "        \n",
    "        # Create optimized system message\n",
    "        system_message = f\"\"\"You are a professional AI assistant representing a skilled data scientist and ML engineer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Guidelines:\n",
    "- Be specific and enthusiastic about their experience\n",
    "- Reference concrete examples from the context\n",
    "- Keep responses focused and under 150 words\n",
    "- Highlight unique value propositions\"\"\"\n",
    "        \n",
    "        # Minimal chat history for performance\n",
    "        messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "        \n",
    "        # Only last exchange for context\n",
    "        if chat_history:\n",
    "            messages.extend(chat_history[-2:])\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        \n",
    "        try:\n",
    "            response = self.client.predict(\n",
    "                endpoint=Config.FOUNDATION_MODEL_ENDPOINT,\n",
    "                inputs={\n",
    "                    \"messages\": messages,\n",
    "                    \"max_tokens\": 200,\n",
    "                    \"temperature\": 0.7\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            response_text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "            # Cache the response\n",
    "            self.rag.cache_response(user_query, chat_history, response_text)\n",
    "            \n",
    "            # Track analytics\n",
    "            response_time = time.time() - start_time\n",
    "            self.rag.track_query_analytics(user_query, response_time, context_tokens)\n",
    "            \n",
    "            return response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"I apologize, but I'm having trouble processing your request. Error: {str(e)}\"\n",
    "    \n",
    "    def get_performance_stats(self) -> Dict:\n",
    "        \"\"\"Get performance statistics\"\"\"\n",
    "        if not self.rag.query_analytics:\n",
    "            return {\"message\": \"No queries processed yet\"}\n",
    "        \n",
    "        response_times = [q[\"response_time\"] for q in self.rag.query_analytics]\n",
    "        context_tokens = [q[\"context_tokens\"] for q in self.rag.query_analytics]\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": len(self.rag.query_analytics),\n",
    "            \"avg_response_time\": np.mean(response_times),\n",
    "            \"avg_context_tokens\": np.mean(context_tokens),\n",
    "            \"cache_hits\": len(self.rag.response_cache),\n",
    "            \"fastest_response\": min(response_times),\n",
    "            \"slowest_response\": max(response_times)\n",
    "        }\n",
    "\n",
    "# Usage in Streamlit app\n",
    "def main_optimized():\n",
    "    st.set_page_config(\n",
    "        page_title=\"Optimized Professional AI Assistant\",\n",
    "        page_icon=\"⚡\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    \n",
    "    st.title(\"⚡ Performance-Optimized Professional AI Assistant\")\n",
    "    st.markdown(\"*Powered by Databricks with Smart Context Management*\")\n",
    "    \n",
    "    # Initialize optimized chatbot\n",
    "    if 'optimized_chatbot' not in st.session_state:\n",
    "        with st.spinner(\"Loading optimized knowledge base...\"):\n",
    "            st.session_state.optimized_chatbot = OptimizedChatBot()\n",
    "    \n",
    "    # Performance metrics in sidebar\n",
    "    with st.sidebar:\n",
    "        st.header(\"Performance Metrics\")\n",
    "        stats = st.session_state.optimized_chatbot.get_performance_stats()\n",
    "        \n",
    "        if \"total_queries\" in stats:\n",
    "            st.metric(\"Total Queries\", stats[\"total_queries\"])\n",
    "            st.metric(\"Avg Response Time\", f\"{stats['avg_response_time']:.2f}s\")\n",
    "            st.metric(\"Avg Context Tokens\", f\"{stats['avg_context_tokens']:.0f}\")\n",
    "            st.metric(\"Cache Hits\", stats[\"cache_hits\"])\n",
    "    \n",
    "    # Rest of the Streamlit app logic...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_optimized()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "streamlitTeseting",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
